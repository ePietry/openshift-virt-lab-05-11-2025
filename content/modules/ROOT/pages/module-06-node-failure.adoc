= Ensuring High Availability with Node Failure Detection and Remediation

== Introduction

In our previous modules, we've built applications with multiple, load-balanced VMs. But what about critical, single-instance workloads? Imagine a legacy **license server** or a specialized, stateful database that cannot be horizontally scaled. If the physical OpenShift node hosting this critical VM experiences a hardware failure or kernel panic, the VM goes down with it. How do we automatically recover?

This scenario requires a "self-healing" cluster. In this module, you will learn how OpenShift Virtualization achieves this using a powerful combination of operators. We will intentionally pin a critical VM to a specific node, and then simulate a catastrophic failure of that node.

You will observe how the **Node Health Check (NHC)** operator detects the failure, how the **Self-Node Remediation (SNR)** operator fences the "dead" node, and how KubeVirt's **Eviction Strategy** automatically restarts your critical VM on a healthy node, ensuring business continuity with minimal downtime.

== Credentials for the Red Hat OpenShift Console

Your OpenShift cluster console is available {openshift_cluster_console_url}[here^].

Your local admin login is available with the following credentials:

* *User:* `{openshift_cluster_admin_username}`
* *Password:* `{openshift_cluster_admin_password}`

You will first see a page that asks you to choose an authentication provider, click on *htpasswd_provider*.

image::module-04-ha/00-htpasswd_login.png[title="OpenShift Authentication", link=self, window=blank, width=100%]

You will then be presented with a login screen where you can copy/paste your credentials.

image::module-04-ha/01-openshift_login.png[title="OpenShift Login", link=self, window=blank, width=100%]

[[observe-ha-config]]
== Observe the High Availability Configuration

This automatic remediation is not magic; it's a pre-configured, coordinated process. Let's briefly observe the key components that are already set up in your cluster.

=== 1. The "Detector": Node Health Check (NHC)

The Node Health Check operator is the "watchdog." It constantly monitors the health of nodes.

. In the *Administrator* perspective, navigate to *Administration* -> *CustomResourceDefinitions*.
. In the filter box, type `NodeHealthCheck` and click the name.
. Click the *Instances* tab. You should see a pre-configured instance (e.g., `nhc-worker`).
. Click on the instance and view its *YAML*.
. Notice the `selector` (it's watching worker nodes) and the `unhealthyConditions`. It's configured to trigger if a node remains `NotReady` for a period (e.g., `2m`). When it triggers, it adds a "remediation-required" taint.
+
image::module-04-ha/02-nhc-yaml.png[title="NodeHealthCheck YAML", link=self, window=blank, width=100%]

=== 2. The "Remediator": Self Node Remediation (SNR)

The Self Node Remediation operator is the "surgeon." It watches for the taint added by the NHC and performs the "fencing" operation.

. Go back to *Administration* -> *CustomResourceDefinitions*.
. Filter for `SelfNodeRemediationConfig` and click it.
. Click the *Instances* tab to see the active configuration (e.g., `snr-config-worker`).
. This operator is configured to watch for the NHC's taint and, when it sees it, will ensure the node is truly "dead" before allowing OpenShift to forcibly evict all of its workloads.

=== 3. The "VM Policy": KubeVirt Eviction Strategy

This is the most critical part for us. KubeVirt needs to know *what* to do with a VM when its node is fenced.

. Go back to *Administration* -> *CustomResourceDefinitions*.
. Filter for `KubeVirt` and click it.
. Go to the *Instances* tab and click the `kubevirt` instance.
. Click the *YAML* tab and scroll down to `spec.configuration`.
. Find the `evictionStrategy`. In this lab, it is set to `Restart`.
+
[source,yaml]
----
spec:
  configuration:
    evictionStrategy: Restart <1>
----
<1> This tells KubeVirt to automatically restart any VM whose node has been fenced. This setting is powerful enough to *override* scheduling constraints like a Node Selector, which is what we will test.

[[create-vm-pinned]]
== Create a VM Pinned to a Specific Node

To properly test our failure scenario, we must ensure our VM is running on a *specific node* that we plan to fail. We will use a **Node Selector** to do this.

. First, let's identify our target node. Go to *Compute* -> *Nodes*.
. Identify a worker node you will use for this test, for example, *worker-5*. Click on it.
. On the *Details* page, find the `kubernetes.io/hostname` label and copy its value (e.g., `worker-5.example.com`).
+
image::module-04-ha/03-node-hostname-label.png[title="Node Hostname Label", link=self, window=blank, width=100%]
+
. Now, let's create a new project. Go to *Projects* -> *Create Project*, name it `critical-vm`, and click *Create*.
. Switch to the *Virtualization* perspective.
. Go to *Virtualization* -> *VirtualMachines* (in the `critical-vm` project).
. Click *Create* -> *VirtualMachine*.
. *Name:* `critical-license-server`
. *Operating System:* Select a *Fedora* or *RHEL* image.
. *Flavor:* Select `tiny`.
. Click on the *Scheduling* tab.
. Expand the *Node Selector* section.
. Click *Add*.
    * *Key:* `kubernetes.io/hostname`
    * *Value:* `worker-5.example.com` (or the node hostname you copied).
+
image::module-04-ha/04-vm-node-selector.png[title="VM Node Selector", link=self, window=blank, width=100%]
+
. Click *Create VirtualMachine*.
. Wait for the VM to start. Once it's *Running*, click on it.
. On the *Overview* tab, verify in the *Details* pane that the *Node* is, in fact, `worker-5.example.com`.
+
image::module-04-ha/05-vm-details-node.png[title="VM Running on Pinned Node", link=self, window=blank, width=100%]
+
Our trap is set. The critical VM is now tied to a single, specific node.

[[simulate-node-failure]]
== Simulate a Node Failure

We cannot physically unplug the node, but we can simulate a catastrophic failure by stopping the `kubelet` (the node's brain) and `crio` (the container runtime). This will make the node stop sending heartbeat signals, appearing "dead" to the cluster.

. In the top right of the OpenShift console, click the *Command line terminal* icon (>_).
+
image::module-04-ha/06-web-terminal.png[title="OpenShift Web Terminal", link=self, window=blank, width=100%]
+
. In the terminal, we will start a privileged "debug" pod on our target node. Replace `worker-5.example.com` with your node's name.
+
[source,sh,role=execute]
----
oc debug node/worker-5.example.com
----
+
. You will see a new prompt, `sh-4.4#` or similar. You are now in a shell *on that node*.
. Enter the node's root filesystem so you can run system commands.
+
[source,sh,role=execute]
----
chroot /host
----
+
. Now, simulate the failure by stopping the `kubelet` and `crio` services.
+
[source,sh,role=execute]
----
systemctl stop kubelet
systemctl stop crio
----
+
. Type `exit` twice, once to leave the `chroot` and once to leave the debug pod. You will be back at your normal `$` prompt.

[[observe-remediation]]
== Observe Automatic VM Failover

The clock is ticking. Let's watch the self-healing process unfold.

. In the OpenShift console, navigate to *Compute* -> *Nodes*.
. Watch your target node (`worker-5.example.com`). After about one minute, its status will change to `NotReady`.
+
image::module-04-ha/07-node-notready.png[title="Node NotReady", link=self, window=blank, width=100%]
+
. The *Node Health Check* operator has now detected this. It is waiting for its `unhealthyConditions` duration to elapse (e.g., 2 minutes) before taking action.
. After the configured time, the *Self Node Remediation* operator will kick in. It will add a "fencing" taint to the node (e.g., `remediation.medik.io/reboot-requested`).
. This taint signals to OpenShift that the node is "dead" and all its workloads must be evicted.
. Now, quickly go to *Virtualization* -> *VirtualMachines*.
. You will see your `critical-license-server` VM briefly enter an `Error` or `Terminating` state.
. Behind the scenes, KubeVirt sees the eviction and consults its `evictionStrategy`. The strategy is `Restart`, so KubeVirt's controller immediately creates a *new* pod for the VM.
. The Kubernetes scheduler, seeing the VM pod needs to run, *ignores* the node selector because the target node is `NotReady` and unschedulable. It places the VM on the next available *healthy* worker.
. You will see the VM go to *Starting* and then *Running*.
. Click on the `critical-license-server` VM again.
. On the *Overview* tab, look at the *Details* pane. The *Node* will **no longer** be `worker-5.example.com`! It will be a different, healthy node (e.g., `worker-6.example.com`).
+
image::module-04-ha/08-vm-failed-over.png[title="VM Restarted on New Node", link=self, window=blank, width=100%]
+
The high-availability system worked. The critical VM was automatically recovered on a healthy node, even though it was "pinned" to the failed node.

[[cleanup]]
== Clean Up the Failed Node

Our test is complete, but our cluster is still short one node. Let's bring the "failed" node back online.

. Open the *Web Terminal* again.
. Start a debug session on the failed node again.
+
[source,sh,role=execute]
----
oc debug node/worker-5.example.com
----
+
. Enter the node's root filesystem.
+
[source,sh,role=execute]
----
chroot /host
----
+
. Restart the services we stopped.
+
[source,sh,role=execute]
----
systemctl start crio
systemctl start kubelet
----
+
. Type `exit` twice to leave the debug pod.
. Go back to *Compute* -> *Nodes*.
. After a few minutes, the `SelfNodeRemediation` operator will detect that the node is healthy again and will remove its taints. The node's status will return to `Ready`.
. The cluster is now back to a fully healthy state.

== Summary

In this module, you simulated a catastrophic node failure to test OpenShift Virtualization's self-healing capabilities. You saw how a combination of operators work together to ensure VM high availability.

* The **Node Health Check (NHC)** operator acted as the "watchdog" to detect the `NotReady` node.
* The **Self-Node Remediation (SNR)** operator acted as the "surgeon" to fence the dead node and evict its workloads.
* The **KubeVirt `evictionStrategy`** provided the final, critical instruction: to `Restart` the evicted VM on a new, healthy node, intelligently overriding the VM's node selector to ensure recovery.

You have successfully demonstrated that even single-instance, stateful VMs can be made highly available in OpenShift Virtualization.