= Exposing Virtual Machines with a LoadBalancer Service

== Introduction

In our previous modules, we've seen how to connect individual VMs to specific networks. Now, we need to address a new requirement: **High Availability**. Our e-commerce application's web front-end cannot rely on a single VM. We need to run multiple instances of our web server VM and distribute incoming customer traffic across them.

This creates a new challenge: how do we provide a *single, stable IP address* for our customers to access, while balancing the traffic between two or more "backend" VMs?

In this module, you will learn how to use standard Kubernetes concepts to solve this. You will first create two identical web server VMs using the *clone* feature. You'll then group them using a Kubernetes **Service** and its **label selector**. Finally, you will expose that Service to the outside world using a `LoadBalancer` type, which in our lab cluster is provided by **MetalLB**. You will then simulate a VM failure and watch the Service automatically redirect traffic, ensuring our application stays online.

== Credentials for the Red Hat OpenShift Console

Your OpenShift cluster console is available {openshift_cluster_console_url}[here^].

Your local admin login is available with the following credentials:

* *User:* `{openshift_cluster_admin_username}`
* *Password:* `{openshift_cluster_admin_password}`

You will first see a page that asks you to choose an authentication provider, click on *htpasswd_provider*.

image::module-03-loadbalancer/00-htpasswd_login.png[title="OpenShift Authentication", link=self, window=blank, width=100%]

You will then be presented with a login screen where you can copy/paste your credentials.

image::module-03-loadbalancer/01-openshift_login.png[title="OpenShift Login", link=self, window=blank, width=100%]

[[vm-create-ha]]
== Create and Label the Virtual Machines

The key to grouping any resources in Kubernetes—whether pods or VMs—is **labels**. We will create two VMs and give them both the same label.

NOTE: For this lab, we will use a pre-built Fedora image that is assumed to have a simple web server (nginx) running on port 80. In a real-world scenario, you would use your own RHEL, Windows, or custom-built template.

. In the left navigation menu, switch to the *Administrator* perspective.
. Click on *Projects* -> *Create Project*.
. Enter the name `web-app-ha` and click *Create*.
. In the left navigation menu, switch to the *Virtualization* perspective.
. Click on *Virtualization* -> *VirtualMachines*. Make sure the `web-app-ha` project is selected.
. Click *Create* -> *VirtualMachine*.
. In the wizard, fill in the *General* section:
    * *Name:* `web-vm-1`
    * *Operating System:* Select a *Fedora* or *RHEL* image.
    * *Flavor:* Select `tiny`.
. *Do not* click Create yet. Click on the *Details* tab (or scroll down to *Labels*).
. Under *Labels*, in the filter box, type `app=my-web-app` and press Enter. This creates a new label key `app` with the value `my-web-app`. This is the most important step.
+
image::module-03-loadbalancer/02-vm-add-label.png[title="Add Label to VM", link=self, window=blank, width=100%]
+
. Click the *Create VirtualMachine* button at the bottom and wait for the VM to boot up and enter the *Running* state.

=== Clone the Virtual Machine

Now that we have one labeled VM, we can easily create our second instance by cloning the first. The clone will automatically inherit the same label.

. On the *VirtualMachines* list, find `web-vm-1`.
. Click the *Kebab menu (⋮)* on the right side of the row, and select *Clone*.
+
image::module-03-loadbalancer/03-vm-clone-menu.png[title="Clone VM Menu", link=self, window=blank, width=100%]
+
. A *Clone Virtual Machine* dialog will appear.
. Change the *Name* to `web-vm-2`.
. Check the box *Start this virtual machine after cloning*.
+
image::module-03-loadbalancer/04-vm-clone-dialog.png[title="Clone VM Dialog", link=self, window=blank, width=100%]
+
. Click *Clone*.
. After a few moments, both `web-vm-1` and `web-vm-2` will be in the *Running* state. If you click on `web-vm-2` and check its *Details* tab, you will see it has the same `app=my-web-app` label as the first.

[[service-create]]
== Creating the Kubernetes Service

Now we have two running VMs, both identifiable by the label `app=my-web-app`. We will now create a **Service** that uses this label as a *selector*. The Service will find all resources (VMs, in this case) with that label and add them to its "backend" pool.

. In the left navigation menu, switch back to the *Administrator* perspective.
. Make sure you are still in the `web-app-ha` project.
. Click on *Networking* -> *Services*.
. Click the *Create Service* button.
. In the *Create Service* form, fill in the following:
    * *Name:* `web-app-svc`
. In the *Selector* section:
    * Enter `app` in the *Key* field.
    * Enter `my-web-app` in the *Value* field. This tells the Service to find anything with the label `app=my-web-app`.
+
image::module-03-loadbalancer/05-service-selector.png[title="Service Selector", link=self, window=blank, width=100%]
+
. In the *Ports* section:
    * *Name:* `http`
    * *Protocol:* `TCP`
    * *Port:* `80` (This is the port the *Service* will listen on).
    * *Target Port:* `80` (This is the port on the *VMs* that traffic will be sent to).
+
image::module-03-loadbalancer/06-service-ports.png[title="Service Ports", link=self, window=blank, width=100%]
+
. Click *Create*.
. You will be taken to the *Service details* page. Notice it has a *ClusterIP* (e.g., `172.30.x.x`). This IP is only reachable from *inside* the cluster.
. Scroll down to the *Pods* section. You should see both `web-vm-1` and `web-vm-2` listed. This confirms the selector is working!
+
image::module-03-loadbalancer/07-service-details-pods.png[title="Service Details Pods", link=self, window=blank, width=100%]

[[service-expose-lb]]
== Expose the Service with MetalLB

Our Service works, but it's only internally accessible. To expose it to the outside world, we will change its `type` from `ClusterIP` (the default) to `LoadBalancer`. In our lab, **MetalLB** is configured to monitor for this and will assign an external IP address from its pool.

. On the *Service details* page for `web-app-svc`, click the *Actions* dropdown in the top right, and select *Edit*.
+
image::module-03-loadbalancer/08-service-edit.png[title="Edit Service", link=self, window=blank, width=100%]
+
. You will be taken to the YAML editor for the Service.
. Find the line that says `type: ClusterIP`.
. Change `ClusterIP` to `LoadBalancer`.
+
[source,yaml,subs="quotes,attributes"]
----
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 80
  selector:
    app: my-web-app
  sessionAffinity: None
  type: *LoadBalancer* <1>
----
<1> Change this line from `ClusterIP` to `LoadBalancer`.
+
. Click the *Save* button.
. You will return to the *Service details* page. After 10-20 seconds, the *Location* section will update. You will see an *External IP* (or *Ingress*) entry with a new IP address. This is your publicly accessible IP provided by MetalLB!
+
image::module-03-loadbalancer/09-service-external-ip.png[title="Service External IP", link=self, window=blank, width=100%]
+
. Copy this IP address to your clipboard or a notepad.

[[lb-verify]]
== Verifying the Load Balancing

Let's test our new LoadBalancer. We will use the OpenShift Web Terminal to send repeated requests to our new external IP.

NOTE: As mentioned, we assume the web server on the template VMs is configured to serve a page that includes its own hostname (e.g., "Hello from web-vm-1"). This allows us to see the load balancing in action.

. In the top right of the OpenShift console, click the *Command line terminal* icon (>_). This will open the Web Terminal.
+
image::module-03-loadbalancer/10-web-terminal.png[title="OpenShift Web Terminal", link=self, window=blank, width=100%]
+
. In the terminal, replace `YOUR_EXTERNAL_IP` with the IP address you copied from the previous step. Then, run the following `while` loop:
+
[source,sh,role=execute]
----
while true; do curl -s http://YOUR_EXTERNAL_IP; echo; sleep 1; done
----
+
. You should see the output alternate between your two VMs, proving the load balancing is working:
+
[source,sh]
----
Hello from web-vm-2
Hello from web-vm-1
Hello from web-vm-2
Hello from web-vm-1
...
----

=== Test High Availability (Simulate Failure)

What happens if one of our VMs crashes? The Service should automatically detect this and send all traffic to the healthy VM.

. Keep the Web Terminal open and running the `curl` loop.
. In the main OpenShift console window, navigate back to *Virtualization* -> *VirtualMachines*.
. Find `web-vm-1`, click its *Kebab menu (⋮)*, and select *Stop*.
+
image::module-03-loadbalancer/11-vm-stop.png[title="Stop VM", link=self, window=blank, width=100%]
+
. Click *Stop* to confirm.
. Now, look back at your Web Terminal.
. After a few seconds, the output will stop alternating. It will *only* show responses from `web-vm-2`:
+
[source,sh]
----
Hello from web-vm-2
Hello from web-vm-1
<...a brief pause or a few errors...>
Hello from web-vm-2
Hello from web-vm-2
Hello from web-vm-2
Hello from web-vm-2
...
----
+
. Our application is still online, with zero downtime for the user!
. Now, go back to the *VirtualMachines* list and *Start* `web-vm-1` again.
. After the VM boots up (about 30-60 seconds), watch your terminal. The output will automatically resume load balancing between both VMs.
+
[source,sh]
----
...
Hello from web-vm-2
Hello from web-vm-2
Hello from web-vm-1
Hello from web-vm-2
Hello from web-vm-1
----
+
. You can press *Ctrl+C* in the terminal to stop the loop.

== Summary

In this module, you saw how OpenShift Virtualization leverages core Kubernetes features to provide high availability for stateful, VM-based workloads.

You created two VMs and used a common **label** to identify them as a group. You then created a Kubernetes **Service** that used this label as a **selector** to dynamically find the running VMs. Finally, you changed the Service `type` to `LoadBalancer`, allowing **MetalLB** to assign a stable, external IP address.

You verified that traffic to this single IP was balanced across both VMs, and you proved the system's resilience by stopping one VM and watching the traffic automatically fail over to the healthy instance.