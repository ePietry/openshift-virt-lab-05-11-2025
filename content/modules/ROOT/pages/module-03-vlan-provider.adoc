= Connecting Virtual Machines to VLANs in OpenShift Virtualization

== Introduction

Our journey continues with a critical migration task. We are moving a legacy, multi-tiered application to OpenShift Virtualization. While the web front-end can live on the standard pod network, the application's **backend database** is subject to strict security and compliance rules. It *must* reside on the existing corporate **VLAN 100** to communicate with other protected services in the datacenter. We cannot simply place it on the default cluster network.

This scenario presents a common challenge: how to bridge the gap between modern Kubernetes networking and traditional, VLAN-segmented datacenter networks.

In this module, you will learn how OpenShift Virtualization uses a two-part system to solve this. First, you'll observe the **NodeNetworkConfigurationPolicy (NNCP)**, which tells the cluster *nodes* how to configure their physical hardware. Second, you'll create a **NetworkAttachmentDefinition (NAD)**, which defines a *usable network* for our VMs. Finally, you'll launch a new Virtual Machine with two network cards: one on the default pod network and one connected directly to our secure VLAN.

== Understanding and Observing the Node Network Configuration (NNCP)

Before we can create a network for our VM, the cluster's *nodes* must be physically prepared. This is the job of the **NodeNetworkConfigurationPolicy (NNCP)**.

Think of the NNCP as the **low-level plumbing**. It's a cluster-admin task, usually done once, that tells specific nodes (like our workers) how to configure their network interfaces. For our VLAN scenario, a cluster admin has already created an NNCP that:

1.  Selects the worker nodes.
2.  Takes a physical network interface (e.g., `ens2`).
3.  Creates a new **Linux bridge** (e.g., `br1`) and attaches that physical interface to it.

This `br1` bridge is now "VLAN-aware" and connected to the corporate network. All VM traffic destined for a VLAN will go through this bridge. Let's observe this existing configuration.

. In the OpenShift console, change your perspective from *Administrator* to *Administrator* (if not already set).
. In the left navigation menu, click on *Administration* -> *CustomResourceDefinitions*.
. In the *Filter by name* box, type `NodeNetworkConfigurationPolicy` and click on the resulting name.
+
image::module-02-vlan/02-search-nncp.png[title="Search for NNCP", link=self, window=blank, width=100%]
+
. Click on the *Instances* tab to see all the policies currently defined.
. You should see a policy named something like `worker-vlan-bridge`. Click on it.
. Click on the *YAML* tab to view the policy's definition.
+
image::module-02-vlan/03-nncp-yaml-view.png[title="NNCP YAML View", link=self, window=blank, width=100%]
+
. Observe the `desiredState`. It describes the network configuration that OpenShift will enforce on the nodes. It will look similar to this:

[source,yaml]
----
spec:
  desiredState:
    interfaces:
      - name: br1 <1>
        description: Linux bridge for VLAN traffic
        type: linux-bridge
        state: up
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: ens2 <2>
  nodeSelector:
    node-role.kubernetes.io/worker: "" <3>
----
<1> This defines the new Linux bridge named `br1`. This is the name we'll need for our next step.
<2> This attaches the node's physical NIC `ens2` to the `br1` bridge.
<3> This policy is applied to all nodes with the "worker" role.

Now that we've confirmed the "plumbing" is in place on the nodes, we can create a network that *uses* it.

[[nad-create]]
== Creating the Network Attachment Definition (NAD)

The NNCP prepared the *nodes*, but it didn't create a *network* that our pods or VMs can use. For that, we need a **NetworkAttachmentDefinition (NAD)**.

Think of the NAD as the **"network invitation"**. It's a namespace-scoped resource that defines a specific network. Our NAD will say: "I'm creating a network called 'vlan-100-finance' that uses the `cnv-bridge` type, connects to the `br1` bridge (from the NNCP), and tags all traffic with **VLAN ID 100**."

. In the left navigation menu, click on *Networking* -> *NetworkAttachmentDefinitions*.
. In the *Project* dropdown at the top, select a project where you want your VM to live. For this lab, let's use the *default* project, or create one named `finance-vms`.
. Click the *Create Network Attachment Definition* button.
+
image::module-02-vlan/04-create-nad-button.png[title="Create NAD Button", link=self, window=blank, width=100%]
+
. Fill in the form with the following details:
    * *Name:* `vlan-100-finance`
    * *Description:* `Corporate finance network on VLAN 100`
    * *Network Type:* Select `CNV Linux bridge` from the dropdown. This is the type used by OpenShift Virtualization.
+
. Once you select the type, the form will update. Fill in the new fields:
    * *Bridge Name:* `br1` (This *must* match the bridge name from the NNCP we observed).
    * *VLAN Tag Number:* `100` (This is the specific VLAN ID our database requires).
+
image::module-02-vlan/05-nad-form-fill.png[title="Fill NAD Form", link=self, window=blank, width=100%]
+
NOTE: We are leaving *IP Address Management* blank. This means we are *not* using Kubernetes to assign an IP address. The VM will either get an IP from the corporate DHCP server on VLAN 100 or we will configure a static IP from *inside* the guest operating system, just like a traditional bare-metal server.

. Click the *Create* button.
. You will see your new `vlan-100-finance` NAD in the list. If you click on it and go to the *YAML* tab, you'll see the resulting configuration:

[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: vlan-100-finance
  namespace: default
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "vlan-100-finance",
    "type": "cnv-bridge", <1>
    "bridge": "br1", <2>
    "vlan": 100, <3>
    "ipam": {} <4>
  }'
----
<1> The network plugin type.
<2> The Linux bridge on the node to use.
<3> The VLAN tag to apply to all traffic.
<4> The empty IPAM block, confirming no cluster-side IP assignment.

[[vm-create]]
== Creating a VM with Multiple Network Interfaces

We're ready to create our database VM. We will attach it to *two* networks:
1.  The default **Pod Network** (for basic cluster connectivity, SSH, etc.).
2.  Our new **vlan-100-finance** network (for secure database traffic).

. In the left navigation menu, switch to the *Virtualization* perspective.
. Click on *Virtualization* -> *VirtualMachines*.
. Make sure you are in the same project where you created the NAD (e.g., *default*).
. Click *Create* -> *VirtualMachine*.
. In the wizard, fill in the *General* section:
    * *Name:* `finance-db-01`
    * *Operating System:* Select a RHEL or Fedora image.
    * *Flavor:* Select `small` or `medium`.
. Click on the *Networking* tab.
+
image::module-02-vlan/06-vm-wizard-networking.png[title="VM Wizard Networking Tab", link=self, window=blank, width=100%]
+
. You will see one interface already present, connected to *Pod Networking*. This is the default.
. Click the *Add Network Interface* button.
. A new modal window will appear. Configure the second interface:
    * *Name:* `nic-1-vlan100` (This is just a friendly name).
    * *Model:* `virtio` (This is the default and recommended).
    * *Network:* Click the dropdown and select our **vlan-100-finance** NAD.
    * *Type:* `Bridge`
    * *MAC Address:* (Leave blank to auto-generate).
+
image::module-02-vlan/07-vm-add-nic-modal.png[title="Add Network Interface Modal", link=self, window=blank, width=100%]
+
. Click the *Add* button on the modal.
. You should now see *two* network interfaces listed for your VM.
+
image::module-02-vlan/08-vm-two-nics.png[title="VM with Two NICs", link=self, window=blank, width=100%]
+
. Click the *Create VirtualMachine* button at the bottom and wait for the VM to boot up.

[[vm-verify]]
== Verifying the VM Network Configuration

Let's confirm that our VM has both network connections.

. Click on the `finance-db-01` VM you just created.
. Go to the *Network Interfaces* tab.
. You will see both interfaces listed:
    * The `Pod Networking` interface will show an IP address assigned by the cluster (e.g., `10.131.x.x`).
    * The `vlan-100-finance` interface will *not* show an IP address. This is expected, as OpenShift is not managing its IP.
+
image::module-02-vlan/09-vm-details-nic-tab.png[title="VM Details NICs", link=self, window=blank, width=100%]
+
. Now, let's verify inside the guest OS.
. Click on the *Console* tab and log in to the VM.
. Once logged in, run the `ip a` command to list all network interfaces.
. You will see (at least) two interfaces, likely `eth0` and `eth1`:
    * `eth0`: This will be the first NIC, connected to the Pod Network. It will have the cluster IP address (e.g., `10.131.5.20`).
    * `eth1`: This will be the second NIC, connected to our VLAN. It will either have no IP *or* an IP from your corporate DHCP server on VLAN 100.
+
[source,sh]
----
$ ip a
...
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> ...
    inet 10.131.5.20/23 ...
...
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> ...
    <no IP address, or one from VLAN 100 DHCP>
----
+
. This confirms the VM is successfully connected to the VLAN. From here, you (or your application team) can log in to the VM and configure a static IP on the `eth1` interface (e.g., `192.168.100.50`) to communicate securely with other services on the finance network.

== Summary

In this module, you successfully provided hybrid networking to a Virtual Machine. You learned the critical difference between the two components that make this possible:

* **NodeNetworkConfigurationPolicy (NNCP):** The low-level, cluster-admin resource that configures the *node's* physical hardware, creating a Linux bridge (`br1`) on a physical NIC.
* **NetworkAttachmentDefinition (NAD):** The high-level, namespace-scoped resource that defines a *usable network* by pointing to the NNCP's bridge (`br1`) and adding a specific VLAN tag (`100`).