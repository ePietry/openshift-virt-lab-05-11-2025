[[udn-create]]
== User-Defined Networks (UDN) : IP Persistantes et Micro-segmentation

Avec les versions récentes d'OpenShift, une nouvelle fonctionnalité puissante est disponible : les **User-Defined Networks (UDN)**.

Dans la section précédente, nous avons utilisé une `NetworkAttachmentDefinition` (NAD) pour nous connecter à un réseau d'entreprise externe (`corp-network`). Cela nous a permis de maintenir une adresse IP stable lors d'une migration à chaud (Live Migration), car l'IP était gérée par un DHCP externe.

Les UDN permettent également de conserver une IP unique et persistante pour nos VMs, mais avec une approche différente : ils utilisent le **SDN (Software-Defined Network) d'OpenShift** lui-même, au lieu de dépendre d'un réseau physique externe.

Un UDN crée un **réseau L2 à plat** qui s'exécute par-dessus l'overlay OpenShift. Il agit comme une "bulle réseau" dédiée pour un ensemble spécifique de VMs.

.Les caractéristiques clés d'un UDN sont :
* **Isolation des flux :** Par défaut, le trafic au sein d'un UDN est complètement isolé. Les ressources connectées à cet UDN ne peuvent communiquer qu'entre elles.
* **IPAM intégré :** L'UDN gère son propre plan d'adressage IP (IPAM). Il distribue des adresses IP persistantes aux VMs, garantissant qu'une VM conservera la même adresse IP même après une migration ou un redémarrage.

---

=== Objectif du Lab

Durant cette partie du lab, nous n'allons pas seulement créer un UDN. Nous allons également mettre en place des **Network Policies** pour aller plus loin dans la segmentation de notre réseau.

[Note]
====
Il est crucial de comprendre que les **Network Policies ne sont pas spécifiques aux UDN**.

Ce sont des ressources Kubernetes standard qui peuvent être utilisées pour contrôler les flux sur n'importe quel réseau géré par OpenShift, y compris :

.   Le SDN OpenShift par défaut (le Pod Network).
.   Les bridges OVS créés par une `NodeNetworkConfigurationPolicy` (NNCP)
.   Et, bien sûr, les UDN que nous nous apprêtons à créer.
====

Voyons comment mettre cela en place.

=== 1. Création d'un User-Defined Network (UDN)

Dans cette approche, nous utilisons les **User-Defined Networks (UDN) natifs d'OVN-Kubernetes**. La création d'un UDN ne se fait pas via une `NetworkAttachmentDefinition` (NAD), mais en combinant deux concepts :

1.  Un objet `UserDefinedNetwork` (CRD) qui définit le réseau L2 lui-même.
2.  Un **label** sur un Namespace, qui désigne cet UDN comme réseau *primaire* pour ce Namespace.

---

==== 1 : Définir l'objet UserDefinedNetwork

D'abord, l'administrateur crée un objet `UserDefinedNetwork` dans le namespace où il sera utilisé. Cet objet définit le nom du réseau, sa topologie (L2) et la gestion des adresses IP (IPAM).

.Exemple de YAML pour un UDN
[source,yaml]
----
apiVersion: k8s.ovn.org/v1
kind: UserDefinedNetwork
metadata:
  name: udn-project-40 <1>
  namespace: udn-project-40 <2>
spec:
  topology: Layer2 <3>
  layer2:
    role: Primary <4>
    subnets:
      - 192.168.1.0/24
  ipam:
    lifecycle: Persistent <5>
----
<1> Le nom de cet objet UDN.
<2> L'UDN doit être créé dans le Namespace qui va l'utiliser.
<3> Définit une topologie L2 à plat (un simple switch).
<4> Indique que ce réseau est conçu pour être un réseau "primaire" (par opposition à un réseau secondaire).
<5> Garantit que les adresses IP des VMs/Pods seront persistantes.

---

==== 2 : Lier le Namespace à l'UDN

Une fois l'objet UDN créé, il faut dire au Namespace de l'utiliser comme réseau primaire par défaut. Cela se fait en appliquant un label spécifique au Namespace.

[source,sh]
----
# L'administrateur applique ce label sur le projet
$ oc label namespace udn-project-40 \
    "k8s.ovn.org/primary-user-defined-network=udn-project-40"
----

<1> Le label `k8s.ovn.org/primary-user-defined-network`...
<2> ...doit pointer vers le `name` de l'objet UDN créé à l'étape 1.

---

[IMPORTANT]
.Configuration Spécifique au Laboratoire
====
Pour cet exercice, **un namespace lié à un UDN a été créé pour chaque participant.**

* Votre namespace de travail est `udn-project-X` (où `X` est votre numéro de participant).
* Ce namespace est déjà pré-configuré avec le label et l'objet UDN (similaires aux exemples ci-dessus).
* Par conséquent, toutes les VMs que vous créerez dans `udn-project-X` seront **automatiquement connectées à cet UDN** par défaut (en plus du réseau de Pods).
====

=== 3. Création de VMs sur l'UDN

Créons maintenant deux VMs, `vm-web` et `vm-db`, et attachons-les à cet UDN. Nous leur ajoutons également des **labels** (`app: web` et `app: db`) que nous utiliserons plus tard pour les politiques réseau.

. Basculez sur la perspective *Virtualization* et assurez-vous d'être dans le bon projet (udn-project-X)
. Créez une nouvelle `VirtualMachine` (par ex. `vm-web`).
. Dans l'onglet *Networking* de l'assistant, cliquez sur *Add Network Interface*.
. Dans la modale, pour le champ *Network*, vous verrez maintenant votre UDN `udn-backend` listé sous la section "User-Defined Networks". Sélectionnez-le.
+
image::module-03-udn/01-vm-select-udn.png[title="Sélection d'un UDN lors de la création de la VM", link=self, window=blank, width=100%]
+
. Cliquez sur *Add*.
. Dans l'onglet *YAML*, ajoutez les **labels** `app: web` au pod de la VM, sous `spec.template.metadata.labels`:

[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-web
  labels:
    app: web <1>
spec:
  template:
    metadata:
      labels:
        app: web <1>
...
    spec:
      domain:
...
      networks:
        - name: default
          pod: {}
        - name: udn-backend
          network:
            kind: Network
            name: udn-backend <2>
      interfaces:
        - name: default
          masquerade: {}
        - name: udn-backend
          bridge: {}
          model: virtio
----
<1> Ces **labels** sont cruciaux pour la micro-segmentation.
<2> Notez que nous lions au `kind: Network` (l'UDN) et non à une NAD.

. Répétez ce processus pour une seconde VM nommée `vm-db`, en utilisant le **label** `app: db`.

=== 3. Vérification de la Persistance d'IP (Live Migration)

Démarrons `vm-db` et vérifions son IP. Ensuite, effectuons une **Live Migration** pour prouver que l'IP ne change pas.

. Démarrez la VM `vm-db`.
. Une fois démarrée, vérifiez l'IP qui lui a été attribuée par l'UDN depuis la CLI :
+
[source,bash]
----
$ oc get vmi vm-db -o jsonpath='{.status.interfaces[?(@.name=="udn-backend")].ipAddress}'
192.168.200.12 <1>
----
<1> Notez cette IP (par ex. `192.168.200.12`).
+
. Maintenant, lancez une **Live Migration** de la VM :
+
[source,bash]
----
$ virtctl migrate vm-db
----
+
. Vous pouvez observer la migration dans l'onglet *Events* de la VM. Une fois terminée, la VM s'exécute sur un nœud worker différent.
. Vérifions à nouveau son adresse IP :
+
[source,bash]
----
$ oc get vmi vm-db -o jsonpath='{.status.interfaces[?(@.name=="udn-backend")].ipAddress}'
192.168.200.12
----
+
Vous constaterez que l'adresse IP est **exactement la même**. L'UDN garantit la persistance de l'IP, ce qui est essentiel pour les services de base de données ou les applications qui dépendent d'IP stables.

=== 4. Micro-segmentation avec les NetworkPolicies

Maintenant, sécurisons nos VMs. Notre objectif :
1.  Bloquer *tout* le trafic entrant vers `vm-db` par défaut.
2.  Autoriser *uniquement* `vm-web` à se connecter à `vm-db` sur le port `3306` (MySQL).
3.  Bloquer tous les autres flux (comme SSH ou ICMP/ping) depuis `vm-web`.

Nous faisons cela en utilisant des ressources **`NetworkPolicy`** standard de Kubernetes.

. Dans le menu de gauche, cliquez sur *Networking* -> *NetworkPolicies*.
. Assurez-vous d'être dans le bon projet (où se trouvent vos VMs).
. Cliquez sur *Create Network Policy*.

. **Politique 1 : Deny All**
+
Créez une politique qui sélectionne `vm-db` et refuse tout le trafic entrant (ingress).
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-deny-all
spec:
  podSelector: <1>
    matchLabels:
      app: db
  policyTypes:
    - Ingress
  ingress: [] <2>
----
<1> `podSelector` fonctionne car la VM est gérée par un pod `virt-launcher` qui porte nos **labels**.
<2> Une liste `ingress` vide signifie "ne rien autoriser".
+
. **Politique 2 : Autoriser le flux Web vers BDD**
+
Créez une seconde politique qui autorise le trafic depuis `vm-web` vers `vm-db` *uniquement* sur le port TCP 3306.
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-web-to-db
spec:
  podSelector:
    matchLabels:
      app: db <1>
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: web <2>
      ports:
        - protocol: TCP
          port: 3306 <3>
----
<1> Cette politique s'applique à `vm-db`.
<2> Elle autorise le trafic *provenant* de `vm-web`.
<3> Elle autorise le trafic *uniquement* sur le port TCP 3306.

=== 5. Vérification des Flux Réseau

Testons nos règles. Nous allons nous connecter à `vm-web` et essayer d'atteindre `vm-db` (ex: `192.168.200.12`).

. Ouvrez la console de `vm-web`.
. **Test 1 : Flux non autorisé (ex: PING ou SSH)**
+
Essayez de pinger `vm-db`.
+
[source,sh]
----
$ ping 192.168.200.12
PING 192.168.200.12 (192.168.200.12) 56(84) bytes of data.
...
--- 192.168.200.12 ping statistics ---
4 packets transmitted, 0 received, 100% packet loss, time 3068ms
----
+
*Résultat :* Échec. Le trafic ICMP n'est pas autorisé par notre politique. Tenter un `ssh user@192.168.200.12` échouerait également.

. **Test 2 : Flux autorisé (TCP/3306)**
+
Utilisons un outil comme `telnet` ou `nc` (netcat) pour tester le port 3306.
+
[source,sh]
----
$ nc -v -z -w 3 192.168.200.12 3306
Connection to 192.168.200.12 3306 port [tcp/mysql] succeeded!
----
+
*Résultat :* Succès. La connexion est établie car elle correspond parfaitement à notre **`NetworkPolicy`** `allow-web-to-db`.

Vous avez maintenant mis en place une micro-segmentation granulaire entre vos VMs, en plus de leur fournir des adresses IP persistantes gérées par le cluster, le tout grâce aux UDN.
```